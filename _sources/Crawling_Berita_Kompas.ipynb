{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMbMdKTBAMo6vGZydCR9sVD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Crawling Berita Kompas"],"metadata":{"id":"byaDP73uqDRV"}},{"cell_type":"markdown","source":["# 1. Library Yang Dibutuhkan"],"metadata":{"id":"YIbVe8oVgPfu"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"KLRl1vl_fUoE","executionInfo":{"status":"ok","timestamp":1701736028251,"user_tz":-420,"elapsed":1714,"user":{"displayName":"21-127 ACHMAD ILHAM FIRMANSYAH","userId":"05099073198681594304"}}},"outputs":[],"source":["from bs4 import BeautifulSoup as soup\n","import requests\n","import pandas as pd"]},{"cell_type":"markdown","source":["# 2. Crawling Berita Kompas"],"metadata":{"id":"ox_vFXX7gjTu"}},{"cell_type":"markdown","source":["Crawling data atau web crawling adalah proses ekstraksi informasi dari situs web dengan menggunakan program komputer. Tujuan utama dari web crawling adalah untuk mengumpulkan data dari berbagai sumber di internet.\n","Terdapat berbagai macam library yang dapat digunakan untuk crawling data. Library yang digunakan untuk crawling berita Kompas yaitu Beautiful Soup dan Requests. Beautiful Soup Digunakan untuk mengekstrak informasi dari HTML dan Request digunakan untuk mengirim permintaan HTTP dan mengakses halaman web."],"metadata":{"id":"t2vpw4ckgo5c"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup as soup\n","import pandas as pd\n","\n","csv = {\"Judul\": [], \"Berita\": [], \"Topik\": []}\n","max_berita_per_topik = 100\n","target_jumlah_topik = 4\n","\n","# List of specific topics you want to crawl\n","target_topics = [\"BOLA\", \"EDUKASI\", \"OTOMOTIF\", \"NEWS\"]\n","\n","for i in range(1, 201):\n","    url = \"https://indeks.kompas.com/?page={}\".format(i)\n","    client = requests.get(url)\n","    page_html = client.content\n","    page_soup = soup(page_html, \"html.parser\")\n","    berita = page_soup.findAll(\"div\", {\"class\": \"latest--indeks mt2 clearfix\"})\n","\n","    for h in berita:\n","        r = requests.get(h.select_one('a.article__link')['href'])\n","        page = soup(r.content, \"html.parser\")\n","        halaman_isi = page.select_one(\"div\", {\"class\": \"col-bs10-10\"})\n","\n","        judul_berita = halaman_isi.select_one(\"h1.read__title\")\n","        judul = judul_berita.text if judul_berita else \"\"\n","\n","        isi_berita = halaman_isi.findAll('p')\n","        isi = '\\n'.join([p.get_text() for p in isi_berita[3:-1] if \"Baca juga\" not in p.get_text()])\n","\n","        topik_berita = h.find(\"div\", {\"class\": \"article__list__info\"}).select(\"div.article__subtitle.article__subtitle--inline\")\n","        topik = topik_berita[0].text if topik_berita else \"\"\n","\n","        # Memeriksa apakah topik berita saat ini termasuk dalam topik yang diinginkan\n","        if topik.upper() in target_topics:\n","            # Memeriksa jumlah berita untuk topik tertentu\n","            jumlah_berita_topik_ini = csv[\"Topik\"].count(topik)\n","            if jumlah_berita_topik_ini < max_berita_per_topik:\n","                csv[\"Judul\"].append(judul)\n","                csv[\"Berita\"].append(isi)\n","                csv[\"Topik\"].append(topik)\n","\n","    # Menyimpan data yang ada pada dictionary ke dalam file csv setelah selesai iterasi halaman\n","    data = pd.DataFrame(csv)\n","    data.to_csv(\"data_berita.csv\", index=False)"],"metadata":{"id":"lvoT3u9Ofv-r","executionInfo":{"status":"ok","timestamp":1701738013236,"user_tz":-420,"elapsed":624257,"user":{"displayName":"21-127 ACHMAD ILHAM FIRMANSYAH","userId":"05099073198681594304"}}},"execution_count":11,"outputs":[]}]}