{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmsQGJzq2/eDuVGLZ6qJaP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Crawling Berita Kompas"],"metadata":{"id":"byaDP73uqDRV"}},{"cell_type":"markdown","source":["# 1. Library Yang Dibutuhkan"],"metadata":{"id":"YIbVe8oVgPfu"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"KLRl1vl_fUoE","executionInfo":{"status":"ok","timestamp":1701714865942,"user_tz":-420,"elapsed":1126,"user":{"displayName":"21-127 ACHMAD ILHAM FIRMANSYAH","userId":"05099073198681594304"}}},"outputs":[],"source":["from bs4 import BeautifulSoup as soup\n","import requests\n","import pandas as pd"]},{"cell_type":"markdown","source":["# 2. Crawling Berita Kompas"],"metadata":{"id":"ox_vFXX7gjTu"}},{"cell_type":"markdown","source":["Crawling data atau web crawling adalah proses ekstraksi informasi dari situs web dengan menggunakan program komputer. Tujuan utama dari web crawling adalah untuk mengumpulkan data dari berbagai sumber di internet.\n","Terdapat berbagai macam library yang dapat digunakan untuk crawling data. Library yang digunakan untuk crawling berita Kompas yaitu Beautiful Soup dan Requests. Beautiful Soup Digunakan untuk mengekstrak informasi dari HTML dan Request digunakan untuk mengirim permintaan HTTP dan mengakses halaman web."],"metadata":{"id":"t2vpw4ckgo5c"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup as soup\n","import pandas as pd\n","\n","csv = {\"Judul\": [], \"Berita\": [], \"Topik\": []}\n","\n","for i in range(1, 10):\n","    url = \"https://indeks.kompas.com/?page={}\".format(i)\n","    client = requests.get(url)\n","    page_html = client.content\n","    page_soup = soup(page_html, \"html.parser\")\n","    berita = page_soup.findAll(\"div\", {\"class\": \"latest--indeks mt2 clearfix\"})\n","\n","    for h in berita:\n","        r = requests.get(h.select_one('a.article__link')['href'])\n","        page = soup(r.content, \"html.parser\")\n","        halaman_isi = page.select_one(\"div\", {\"class\": \"col-bs10-10\"})\n","\n","        # Mengambil data judul tugas akhir\n","        judul_berita = halaman_isi.select(\"h1\", {\"class\": \"read__title\"})\n","        judul = judul_berita[0].text\n","\n","        # Mengambil data judul tugas akhir\n","        isi_berita = halaman_isi.findAll('p')\n","\n","        # Exclude the first three paragraphs, the last paragraph, and paragraphs with \"Baca juga\"\n","        isi = '\\n'.join([p.get_text() for p in isi_berita[3:-1] if \"Baca juga\" not in p.get_text()])\n","\n","        # Mengambil data judul tugas akhir\n","        topik_berita = h.find(\"div\", {\"class\": \"article__list__info\"}).select(\"div\", {\"class\": \"article__subtitle article__subtitle--inline\"})\n","        topik = topik_berita[0].text\n","\n","        # Menambahkan data yang sudah diambil ke dalam list\n","        csv[\"Judul\"].append(judul)\n","        csv[\"Berita\"].append(isi)\n","        csv[\"Topik\"].append(topik)\n","\n","        # Menyimpan data yang ada pada dictionary ke dalam file csv\n","        data = pd.DataFrame(csv)\n","        data.to_csv(\"data_berita.csv\", index=False)"],"metadata":{"id":"lvoT3u9Ofv-r","executionInfo":{"status":"ok","timestamp":1701715398328,"user_tz":-420,"elapsed":17388,"user":{"displayName":"21-127 ACHMAD ILHAM FIRMANSYAH","userId":"05099073198681594304"}}},"execution_count":3,"outputs":[]}]}